{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Electrocardiograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Electrocardiograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ Import the [`electrocardiograms.csv`](https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Electrocardiograms_dataset.csv) dataset and display its first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>x_11</th>\n",
       "      <th>x_12</th>\n",
       "      <th>x_13</th>\n",
       "      <th>x_14</th>\n",
       "      <th>x_15</th>\n",
       "      <th>x_16</th>\n",
       "      <th>x_17</th>\n",
       "      <th>x_18</th>\n",
       "      <th>x_19</th>\n",
       "      <th>x_20</th>\n",
       "      <th>x_21</th>\n",
       "      <th>x_22</th>\n",
       "      <th>x_23</th>\n",
       "      <th>x_24</th>\n",
       "      <th>x_25</th>\n",
       "      <th>x_26</th>\n",
       "      <th>x_27</th>\n",
       "      <th>x_28</th>\n",
       "      <th>x_29</th>\n",
       "      <th>x_30</th>\n",
       "      <th>x_31</th>\n",
       "      <th>x_32</th>\n",
       "      <th>x_33</th>\n",
       "      <th>x_34</th>\n",
       "      <th>x_35</th>\n",
       "      <th>x_36</th>\n",
       "      <th>x_37</th>\n",
       "      <th>x_38</th>\n",
       "      <th>x_39</th>\n",
       "      <th>x_40</th>\n",
       "      <th>x_41</th>\n",
       "      <th>x_42</th>\n",
       "      <th>x_43</th>\n",
       "      <th>x_44</th>\n",
       "      <th>x_45</th>\n",
       "      <th>x_46</th>\n",
       "      <th>x_47</th>\n",
       "      <th>x_48</th>\n",
       "      <th>x_49</th>\n",
       "      <th>x_50</th>\n",
       "      <th>x_51</th>\n",
       "      <th>x_52</th>\n",
       "      <th>x_53</th>\n",
       "      <th>x_54</th>\n",
       "      <th>x_55</th>\n",
       "      <th>x_56</th>\n",
       "      <th>x_57</th>\n",
       "      <th>x_58</th>\n",
       "      <th>x_59</th>\n",
       "      <th>x_60</th>\n",
       "      <th>x_61</th>\n",
       "      <th>x_62</th>\n",
       "      <th>x_63</th>\n",
       "      <th>x_64</th>\n",
       "      <th>x_65</th>\n",
       "      <th>x_66</th>\n",
       "      <th>x_67</th>\n",
       "      <th>x_68</th>\n",
       "      <th>x_69</th>\n",
       "      <th>x_70</th>\n",
       "      <th>x_71</th>\n",
       "      <th>x_72</th>\n",
       "      <th>x_73</th>\n",
       "      <th>x_74</th>\n",
       "      <th>x_75</th>\n",
       "      <th>x_76</th>\n",
       "      <th>x_77</th>\n",
       "      <th>x_78</th>\n",
       "      <th>x_79</th>\n",
       "      <th>x_80</th>\n",
       "      <th>x_81</th>\n",
       "      <th>x_82</th>\n",
       "      <th>x_83</th>\n",
       "      <th>x_84</th>\n",
       "      <th>x_85</th>\n",
       "      <th>x_86</th>\n",
       "      <th>x_87</th>\n",
       "      <th>x_88</th>\n",
       "      <th>x_89</th>\n",
       "      <th>x_90</th>\n",
       "      <th>x_91</th>\n",
       "      <th>x_92</th>\n",
       "      <th>x_93</th>\n",
       "      <th>x_94</th>\n",
       "      <th>x_95</th>\n",
       "      <th>x_96</th>\n",
       "      <th>x_97</th>\n",
       "      <th>x_98</th>\n",
       "      <th>x_99</th>\n",
       "      <th>x_100</th>\n",
       "      <th>x_101</th>\n",
       "      <th>x_102</th>\n",
       "      <th>x_103</th>\n",
       "      <th>x_104</th>\n",
       "      <th>x_105</th>\n",
       "      <th>x_106</th>\n",
       "      <th>x_107</th>\n",
       "      <th>x_108</th>\n",
       "      <th>x_109</th>\n",
       "      <th>x_110</th>\n",
       "      <th>x_111</th>\n",
       "      <th>x_112</th>\n",
       "      <th>x_113</th>\n",
       "      <th>x_114</th>\n",
       "      <th>x_115</th>\n",
       "      <th>x_116</th>\n",
       "      <th>x_117</th>\n",
       "      <th>x_118</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "      <th>x_128</th>\n",
       "      <th>x_129</th>\n",
       "      <th>x_130</th>\n",
       "      <th>x_131</th>\n",
       "      <th>x_132</th>\n",
       "      <th>x_133</th>\n",
       "      <th>x_134</th>\n",
       "      <th>x_135</th>\n",
       "      <th>x_136</th>\n",
       "      <th>x_137</th>\n",
       "      <th>x_138</th>\n",
       "      <th>x_139</th>\n",
       "      <th>x_140</th>\n",
       "      <th>x_141</th>\n",
       "      <th>x_142</th>\n",
       "      <th>x_143</th>\n",
       "      <th>x_144</th>\n",
       "      <th>x_145</th>\n",
       "      <th>x_146</th>\n",
       "      <th>x_147</th>\n",
       "      <th>x_148</th>\n",
       "      <th>x_149</th>\n",
       "      <th>x_150</th>\n",
       "      <th>x_151</th>\n",
       "      <th>x_152</th>\n",
       "      <th>x_153</th>\n",
       "      <th>x_154</th>\n",
       "      <th>x_155</th>\n",
       "      <th>x_156</th>\n",
       "      <th>x_157</th>\n",
       "      <th>x_158</th>\n",
       "      <th>x_159</th>\n",
       "      <th>x_160</th>\n",
       "      <th>x_161</th>\n",
       "      <th>x_162</th>\n",
       "      <th>x_163</th>\n",
       "      <th>x_164</th>\n",
       "      <th>x_165</th>\n",
       "      <th>x_166</th>\n",
       "      <th>x_167</th>\n",
       "      <th>x_168</th>\n",
       "      <th>x_169</th>\n",
       "      <th>x_170</th>\n",
       "      <th>x_171</th>\n",
       "      <th>x_172</th>\n",
       "      <th>x_173</th>\n",
       "      <th>x_174</th>\n",
       "      <th>x_175</th>\n",
       "      <th>x_176</th>\n",
       "      <th>x_177</th>\n",
       "      <th>x_178</th>\n",
       "      <th>x_179</th>\n",
       "      <th>x_180</th>\n",
       "      <th>x_181</th>\n",
       "      <th>x_182</th>\n",
       "      <th>x_183</th>\n",
       "      <th>x_184</th>\n",
       "      <th>x_185</th>\n",
       "      <th>x_186</th>\n",
       "      <th>x_187</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.322097</td>\n",
       "      <td>0.363296</td>\n",
       "      <td>0.413858</td>\n",
       "      <td>0.426966</td>\n",
       "      <td>0.485019</td>\n",
       "      <td>0.511236</td>\n",
       "      <td>0.520599</td>\n",
       "      <td>0.548689</td>\n",
       "      <td>0.599251</td>\n",
       "      <td>0.606742</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.664794</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.852060</td>\n",
       "      <td>0.897004</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>0.970037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992509</td>\n",
       "      <td>0.985019</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.823970</td>\n",
       "      <td>0.752809</td>\n",
       "      <td>0.711610</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.602996</td>\n",
       "      <td>0.576779</td>\n",
       "      <td>0.597378</td>\n",
       "      <td>0.670412</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.513109</td>\n",
       "      <td>0.423221</td>\n",
       "      <td>0.277154</td>\n",
       "      <td>0.119850</td>\n",
       "      <td>0.082397</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.039326</td>\n",
       "      <td>0.054307</td>\n",
       "      <td>0.063670</td>\n",
       "      <td>0.198502</td>\n",
       "      <td>0.303371</td>\n",
       "      <td>0.355805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.610714</td>\n",
       "      <td>0.466071</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.364286</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.305357</td>\n",
       "      <td>0.308929</td>\n",
       "      <td>0.305357</td>\n",
       "      <td>0.291071</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.283929</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>0.264286</td>\n",
       "      <td>0.260714</td>\n",
       "      <td>0.251786</td>\n",
       "      <td>0.241071</td>\n",
       "      <td>0.226786</td>\n",
       "      <td>0.217857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.173214</td>\n",
       "      <td>0.164286</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.155357</td>\n",
       "      <td>0.141071</td>\n",
       "      <td>0.144643</td>\n",
       "      <td>0.155357</td>\n",
       "      <td>0.167857</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.192857</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.251786</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>0.276786</td>\n",
       "      <td>0.310714</td>\n",
       "      <td>0.323214</td>\n",
       "      <td>0.323214</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.348214</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.341071</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.358929</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.308929</td>\n",
       "      <td>0.360714</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.205357</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.048214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041071</td>\n",
       "      <td>0.101786</td>\n",
       "      <td>0.146429</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.246429</td>\n",
       "      <td>0.301786</td>\n",
       "      <td>0.351786</td>\n",
       "      <td>0.382143</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.398214</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.421429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.994200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951276</td>\n",
       "      <td>0.903712</td>\n",
       "      <td>0.917633</td>\n",
       "      <td>0.900232</td>\n",
       "      <td>0.803944</td>\n",
       "      <td>0.656613</td>\n",
       "      <td>0.421114</td>\n",
       "      <td>0.288863</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.269142</td>\n",
       "      <td>0.244780</td>\n",
       "      <td>0.230858</td>\n",
       "      <td>0.216937</td>\n",
       "      <td>0.209977</td>\n",
       "      <td>0.206497</td>\n",
       "      <td>0.193735</td>\n",
       "      <td>0.187935</td>\n",
       "      <td>0.179814</td>\n",
       "      <td>0.177494</td>\n",
       "      <td>0.160093</td>\n",
       "      <td>0.142691</td>\n",
       "      <td>0.133411</td>\n",
       "      <td>0.132251</td>\n",
       "      <td>0.121810</td>\n",
       "      <td>0.107889</td>\n",
       "      <td>0.106729</td>\n",
       "      <td>0.113689</td>\n",
       "      <td>0.096288</td>\n",
       "      <td>0.075406</td>\n",
       "      <td>0.066125</td>\n",
       "      <td>0.048724</td>\n",
       "      <td>0.022042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.024362</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.067285</td>\n",
       "      <td>0.112529</td>\n",
       "      <td>0.155452</td>\n",
       "      <td>0.196056</td>\n",
       "      <td>0.220418</td>\n",
       "      <td>0.241299</td>\n",
       "      <td>0.256380</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>0.262181</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>0.259861</td>\n",
       "      <td>0.261021</td>\n",
       "      <td>0.269142</td>\n",
       "      <td>0.265661</td>\n",
       "      <td>0.263341</td>\n",
       "      <td>0.263341</td>\n",
       "      <td>0.271462</td>\n",
       "      <td>0.270302</td>\n",
       "      <td>0.270302</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.271462</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>0.277262</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.285383</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.280742</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.287703</td>\n",
       "      <td>0.286543</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.287703</td>\n",
       "      <td>0.291183</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.291183</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.291183</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.298144</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.306264</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.309745</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.302784</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.305104</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.309745</td>\n",
       "      <td>0.309745</td>\n",
       "      <td>0.317865</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.317865</td>\n",
       "      <td>0.319026</td>\n",
       "      <td>0.328306</td>\n",
       "      <td>0.341067</td>\n",
       "      <td>0.352668</td>\n",
       "      <td>0.37007</td>\n",
       "      <td>0.390951</td>\n",
       "      <td>0.385151</td>\n",
       "      <td>0.387471</td>\n",
       "      <td>0.37587</td>\n",
       "      <td>0.338747</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.402552</td>\n",
       "      <td>0.62181</td>\n",
       "      <td>0.790023</td>\n",
       "      <td>0.75174</td>\n",
       "      <td>0.468677</td>\n",
       "      <td>0.267981</td>\n",
       "      <td>0.349188</td>\n",
       "      <td>0.356148</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.305104</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.302784</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.984472</td>\n",
       "      <td>0.962733</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.100932</td>\n",
       "      <td>0.177019</td>\n",
       "      <td>0.270186</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.316770</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.316770</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.322981</td>\n",
       "      <td>0.330745</td>\n",
       "      <td>0.343168</td>\n",
       "      <td>0.355590</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.406832</td>\n",
       "      <td>0.406832</td>\n",
       "      <td>0.411491</td>\n",
       "      <td>0.405280</td>\n",
       "      <td>0.395963</td>\n",
       "      <td>0.377329</td>\n",
       "      <td>0.377329</td>\n",
       "      <td>0.378882</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.363354</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.361801</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.363354</td>\n",
       "      <td>0.361801</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.371118</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.378882</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.371118</td>\n",
       "      <td>0.375776</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.364907</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.374224</td>\n",
       "      <td>0.371118</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.389752</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.408385</td>\n",
       "      <td>0.416149</td>\n",
       "      <td>0.439441</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.470497</td>\n",
       "      <td>0.451863</td>\n",
       "      <td>0.459627</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.427019</td>\n",
       "      <td>0.399068</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.363354</td>\n",
       "      <td>0.364907</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.364907</td>\n",
       "      <td>0.388199</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.734472</td>\n",
       "      <td>0.911491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.614907</td>\n",
       "      <td>0.406832</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.349379</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.298137</td>\n",
       "      <td>0.301242</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.298137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.619217</td>\n",
       "      <td>0.489324</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.110320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060498</td>\n",
       "      <td>0.108541</td>\n",
       "      <td>0.108541</td>\n",
       "      <td>0.145907</td>\n",
       "      <td>0.192171</td>\n",
       "      <td>0.222420</td>\n",
       "      <td>0.259786</td>\n",
       "      <td>0.279359</td>\n",
       "      <td>0.282918</td>\n",
       "      <td>0.279359</td>\n",
       "      <td>0.275801</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.286477</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>0.279359</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.297153</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.334520</td>\n",
       "      <td>0.357651</td>\n",
       "      <td>0.380783</td>\n",
       "      <td>0.405694</td>\n",
       "      <td>0.435943</td>\n",
       "      <td>0.464413</td>\n",
       "      <td>0.476868</td>\n",
       "      <td>0.491103</td>\n",
       "      <td>0.508897</td>\n",
       "      <td>0.501779</td>\n",
       "      <td>0.496441</td>\n",
       "      <td>0.483986</td>\n",
       "      <td>0.471530</td>\n",
       "      <td>0.457295</td>\n",
       "      <td>0.430605</td>\n",
       "      <td>0.416370</td>\n",
       "      <td>0.407473</td>\n",
       "      <td>0.386121</td>\n",
       "      <td>0.359431</td>\n",
       "      <td>0.350534</td>\n",
       "      <td>0.354093</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.314947</td>\n",
       "      <td>0.322064</td>\n",
       "      <td>0.318505</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.311388</td>\n",
       "      <td>0.316726</td>\n",
       "      <td>0.311388</td>\n",
       "      <td>0.302491</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>0.307829</td>\n",
       "      <td>0.304270</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.286477</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.297153</td>\n",
       "      <td>0.302491</td>\n",
       "      <td>0.300712</td>\n",
       "      <td>0.309609</td>\n",
       "      <td>0.323843</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.320285</td>\n",
       "      <td>0.314947</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>0.274021</td>\n",
       "      <td>0.266904</td>\n",
       "      <td>0.263345</td>\n",
       "      <td>0.261566</td>\n",
       "      <td>0.263345</td>\n",
       "      <td>0.272242</td>\n",
       "      <td>0.277580</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.354093</td>\n",
       "      <td>0.471530</td>\n",
       "      <td>0.658363</td>\n",
       "      <td>0.850534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976868</td>\n",
       "      <td>0.542705</td>\n",
       "      <td>0.193950</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.218861</td>\n",
       "      <td>0.224199</td>\n",
       "      <td>0.201068</td>\n",
       "      <td>0.204626</td>\n",
       "      <td>0.209964</td>\n",
       "      <td>0.201068</td>\n",
       "      <td>0.197509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2       x_3       x_4       x_5       x_6       x_7  \\\n",
       "0  0.000000  0.041199  0.112360  0.146067  0.202247  0.322097  0.363296   \n",
       "1  1.000000  0.901786  0.760714  0.610714  0.466071  0.385714  0.364286   \n",
       "2  0.994200  1.000000  0.951276  0.903712  0.917633  0.900232  0.803944   \n",
       "3  0.984472  0.962733  0.663043  0.211180  0.000000  0.032609  0.100932   \n",
       "4  0.619217  0.489324  0.327402  0.110320  0.000000  0.060498  0.108541   \n",
       "\n",
       "        x_8       x_9      x_10      x_11      x_12      x_13      x_14  \\\n",
       "0  0.413858  0.426966  0.485019  0.511236  0.520599  0.548689  0.599251   \n",
       "1  0.346429  0.314286  0.305357  0.308929  0.305357  0.291071  0.285714   \n",
       "2  0.656613  0.421114  0.288863  0.290023  0.269142  0.244780  0.230858   \n",
       "3  0.177019  0.270186  0.313665  0.307453  0.312112  0.312112  0.313665   \n",
       "4  0.108541  0.145907  0.192171  0.222420  0.259786  0.279359  0.282918   \n",
       "\n",
       "       x_15      x_16      x_17      x_18      x_19      x_20      x_21  \\\n",
       "0  0.606742  0.640449  0.664794  0.730337  0.780899  0.852060  0.897004   \n",
       "1  0.283929  0.271429  0.255357  0.264286  0.260714  0.251786  0.241071   \n",
       "2  0.216937  0.209977  0.206497  0.193735  0.187935  0.179814  0.177494   \n",
       "3  0.315217  0.319876  0.316770  0.312112  0.313665  0.319876  0.316770   \n",
       "4  0.279359  0.275801  0.281139  0.288256  0.286477  0.281139  0.279359   \n",
       "\n",
       "       x_22      x_23      x_24      x_25      x_26      x_27      x_28  \\\n",
       "0  0.953184  0.970037  1.000000  0.992509  0.985019  0.943820  0.898876   \n",
       "1  0.226786  0.217857  0.200000  0.173214  0.164286  0.160714  0.155357   \n",
       "2  0.160093  0.142691  0.133411  0.132251  0.121810  0.107889  0.106729   \n",
       "3  0.307453  0.313665  0.315217  0.315217  0.322981  0.330745  0.343168   \n",
       "4  0.290036  0.291815  0.297153  0.313167  0.334520  0.357651  0.380783   \n",
       "\n",
       "       x_29      x_30      x_31      x_32      x_33      x_34      x_35  \\\n",
       "0  0.823970  0.752809  0.711610  0.666667  0.602996  0.576779  0.597378   \n",
       "1  0.141071  0.144643  0.155357  0.167857  0.175000  0.192857  0.223214   \n",
       "2  0.113689  0.096288  0.075406  0.066125  0.048724  0.022042  0.000000   \n",
       "3  0.355590  0.366460  0.380435  0.394410  0.406832  0.406832  0.411491   \n",
       "4  0.405694  0.435943  0.464413  0.476868  0.491103  0.508897  0.501779   \n",
       "\n",
       "       x_36      x_37      x_38      x_39      x_40      x_41      x_42  \\\n",
       "0  0.670412  0.595506  0.513109  0.423221  0.277154  0.119850  0.082397   \n",
       "1  0.251786  0.255357  0.276786  0.310714  0.323214  0.323214  0.326786   \n",
       "2  0.002320  0.024362  0.046404  0.067285  0.112529  0.155452  0.196056   \n",
       "3  0.405280  0.395963  0.377329  0.377329  0.378882  0.369565  0.363354   \n",
       "4  0.496441  0.483986  0.471530  0.457295  0.430605  0.416370  0.407473   \n",
       "\n",
       "       x_43      x_44      x_45      x_46      x_47      x_48      x_49  \\\n",
       "0  0.022472  0.039326  0.054307  0.063670  0.198502  0.303371  0.355805   \n",
       "1  0.342857  0.346429  0.339286  0.342857  0.348214  0.346429  0.335714   \n",
       "2  0.220418  0.241299  0.256380  0.257541  0.252900  0.257541  0.262181   \n",
       "3  0.366460  0.366460  0.361801  0.357143  0.366460  0.369565  0.363354   \n",
       "4  0.386121  0.359431  0.350534  0.354093  0.341637  0.330961  0.327402   \n",
       "\n",
       "       x_50      x_51      x_52      x_53      x_54      x_55      x_56  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.335714  0.339286  0.341071  0.342857  0.357143  0.358929  0.328571   \n",
       "2  0.257541  0.259861  0.261021  0.269142  0.265661  0.263341  0.263341   \n",
       "3  0.361801  0.366460  0.372671  0.371118  0.369565  0.369565  0.378882   \n",
       "4  0.330961  0.327402  0.313167  0.314947  0.322064  0.318505  0.313167   \n",
       "\n",
       "       x_57      x_58      x_59      x_60      x_61      x_62      x_63  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.308929  0.360714  0.455357  0.457143  0.366071  0.205357  0.114286   \n",
       "2  0.271462  0.270302  0.270302  0.273782  0.278422  0.278422  0.271462   \n",
       "3  0.366460  0.357143  0.371118  0.375776  0.372671  0.364907  0.369565   \n",
       "4  0.311388  0.316726  0.311388  0.302491  0.298932  0.307829  0.304270   \n",
       "\n",
       "       x_64      x_65      x_66      x_67      x_68      x_69      x_70  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.048214  0.000000  0.041071  0.101786  0.146429  0.187500  0.246429   \n",
       "2  0.273782  0.278422  0.279582  0.273782  0.277262  0.279582  0.279582   \n",
       "3  0.374224  0.371118  0.372671  0.380435  0.389752  0.394410  0.408385   \n",
       "4  0.295374  0.290036  0.298932  0.291815  0.290036  0.290036  0.295374   \n",
       "\n",
       "       x_71      x_72      x_73      x_74      x_75      x_76      x_77  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.301786  0.351786  0.382143  0.387500  0.398214  0.407143  0.407143   \n",
       "2  0.278422  0.278422  0.285383  0.283063  0.280742  0.283063  0.287703   \n",
       "3  0.416149  0.439441  0.456522  0.470497  0.451863  0.459627  0.453416   \n",
       "4  0.288256  0.290036  0.288256  0.288256  0.288256  0.286477  0.291815   \n",
       "\n",
       "       x_78      x_79      x_80      x_81      x_82      x_83      x_84  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.410714  0.421429  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.286543  0.283063  0.287703  0.291183  0.293503  0.290023  0.291183   \n",
       "3  0.427019  0.399068  0.394410  0.369565  0.354037  0.363354  0.364907   \n",
       "4  0.297153  0.302491  0.300712  0.309609  0.323843  0.330961  0.327402   \n",
       "\n",
       "       x_85      x_86      x_87      x_88      x_89      x_90      x_91  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.296984  0.294664  0.290023  0.290023  0.295824  0.291183  0.293503   \n",
       "3  0.366460  0.364907  0.388199  0.535714  0.734472  0.911491  1.000000   \n",
       "4  0.320285  0.314947  0.295374  0.281139  0.274021  0.266904  0.263345   \n",
       "\n",
       "       x_92      x_93      x_94      x_95      x_96      x_97      x_98  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.293503  0.300464  0.301624  0.296984  0.294664  0.300464  0.294664   \n",
       "3  0.919255  0.614907  0.406832  0.372671  0.349379  0.315217  0.304348   \n",
       "4  0.261566  0.263345  0.272242  0.277580  0.295374  0.354093  0.471530   \n",
       "\n",
       "       x_99     x_100     x_101     x_102     x_103     x_104     x_105  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.293503  0.299304  0.303944  0.300464  0.294664  0.299304  0.307425   \n",
       "3  0.313665  0.312112  0.304348  0.298137  0.301242  0.307453  0.298137   \n",
       "4  0.658363  0.850534  1.000000  0.976868  0.542705  0.193950  0.185053   \n",
       "\n",
       "      x_106     x_107     x_108     x_109     x_110     x_111     x_112  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.301624  0.300464  0.293503  0.303944  0.303944  0.298144  0.296984   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.218861  0.224199  0.201068  0.204626  0.209964  0.201068  0.197509   \n",
       "\n",
       "      x_113     x_114     x_115     x_116     x_117     x_118     x_119  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.303944  0.306264  0.300464  0.301624  0.309745  0.312065  0.307425   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_120     x_121     x_122     x_123     x_124     x_125     x_126  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.302784  0.310905  0.308585  0.299304  0.301624  0.307425  0.310905   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_127     x_128     x_129     x_130     x_131     x_132     x_133  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.305104  0.308585  0.313225  0.310905  0.309745  0.309745  0.317865   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_134     x_135     x_136     x_137     x_138     x_139     x_140  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.315545  0.310905  0.312065  0.317865  0.319026  0.328306  0.341067   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_141    x_142     x_143     x_144     x_145    x_146     x_147  \\\n",
       "0  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "1  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "2  0.352668  0.37007  0.390951  0.385151  0.387471  0.37587  0.338747   \n",
       "3  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "4  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "\n",
       "      x_148     x_149     x_150     x_151     x_152     x_153     x_154  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.312065  0.308585  0.312065  0.307425  0.301624  0.308585  0.307425   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_155     x_156     x_157     x_158    x_159     x_160    x_161  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "2  0.300464  0.283063  0.301624  0.402552  0.62181  0.790023  0.75174   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "\n",
       "      x_162     x_163     x_164     x_165     x_166     x_167     x_168  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.468677  0.267981  0.349188  0.356148  0.313225  0.295824  0.305104   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_169     x_170     x_171     x_172     x_173     x_174     x_175  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.301624  0.302784  0.294664  0.295824  0.300464  0.296984  0.293503   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_176     x_177     x_178     x_179     x_180     x_181  x_182  x_183  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "2  0.290023  0.296984  0.300464  0.294664  0.295824  0.301624    0.0    0.0   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "\n",
       "   x_184  x_185  x_186  x_187  target  \n",
       "0    0.0    0.0    0.0    0.0       1  \n",
       "1    0.0    0.0    0.0    0.0       1  \n",
       "2    0.0    0.0    0.0    0.0       1  \n",
       "3    0.0    0.0    0.0    0.0       1  \n",
       "4    0.0    0.0    0.0    0.0       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"max_columns\", None)\n",
    "\n",
    "url = \"https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Electrocardiograms_dataset.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’“ Each observation of the dataset is a sequence of measured heartbeats, taken from a patient's electrocardiogram (ECG).\n",
    "\n",
    "ðŸŽ¯ The target is binary and defines whether the heartbeat shows:\n",
    "* a risk of cardiovascular disease ðŸ”´ (1)\n",
    "* or not ðŸŸ¢ (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question** â“\n",
    "\n",
    "Plot an observation of each target class to get a visual idea of what the numbers represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f95e2971d30>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18117"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.sum() - len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Questions** â“\n",
    "\n",
    "* How many observations of at-risk heartbeats are there? Save your answer as `at_risk_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "at_risk_count = 1448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many observations of healthy heartbeats are there? Save your answer as `healthy_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "healthy_count = 18117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘©ðŸ»â€ðŸ« In certain cases, the class balance is representative of the true class distribution. This is the case here: the vast majority of people actually have healthy hearts. In such case, we preserve the class distribution to train the model based on reality, and adapt our modeling approach accordingly.\n",
    "\n",
    "[Centers for Disease Control and Prevention - Heart Disease Facts](https://www.cdc.gov/heartdisease/facts.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/bitazaratustra/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/bitazaratustra/code/bitazaratustra/data-challenges/05-ML/03-Performance-metrics/02-Electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/test_class_balance.py::TestClass_balance::test_at_risk_count \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_class_balance.py::TestClass_balance::test_healthy_count \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/class_balance.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed class_balance step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('class_balance',\n",
    "                         healthy = healthy_count,\n",
    "                         at_risk = at_risk_count)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (3) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ¯ Your task is to **flag heartbeats that are at risk of cardiovascular diseases.**\n",
    "\n",
    "ðŸ‘‡ Let's start by investigating the performance of a `LogisticRegression` on that task. Use a ***cross-validation to evaluate the model*** on the following metrics:\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Precision\n",
    "- F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = data.drop(columns = 'target')\n",
    "y = data['target']\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "cv_results = cross_validate(model, X, y, cv=5, scoring=['accuracy', 'precision', 'f1','recall'], )\n",
    "Accuracy = cv_results['test_accuracy'].mean()\n",
    "Recall = cv_results['test_recall'].mean()\n",
    "Precision = cv_results['test_precision'].mean()\n",
    "F1 = cv_results['test_f1'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Ratio of correct predictions)** â“ \n",
    "\n",
    "What is the ratio of correct predictions for this model ? Save your answer under variable name `correct_pred_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "correct_pred_ratio = Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Ability to flag at-risk patients)** â“ \n",
    "\n",
    "What percentage of at-risk heartbeats is the model able to flag? Save your answer under variable name `flag_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "flag_ratio = Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Ability to flag correctly)** â“ \n",
    "\n",
    "When the model signals an at-risk heartbeat, how often is it correct? Save your answer under variable name `correct_detection_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "correct_detection_ratio = Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Detecting as many at-risk patients as possible without too many false alarms)** â“ \n",
    "\n",
    "What is the model's ability to flag as many at-risk heartbeats as possible while limiting false alarms?  Save your answer under variable name `aggregated_metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "aggregated_metric = F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/bitazaratustra/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/bitazaratustra/code/bitazaratustra/data-challenges/05-ML/03-Performance-metrics/02-Electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "tests/test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_accuracy \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_f1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_precision \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_recall \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/logistic_regression_evaluation.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed logistic_regression_evaluation step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('logistic_regression_evaluation',\n",
    "                         accuracy = correct_pred_ratio,\n",
    "                         recall = flag_ratio,\n",
    "                         precision = correct_detection_ratio,\n",
    "                         f1 = aggregated_metric)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â–¶ï¸ Run the following cell before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should have noticed that the model was able to predict correctly in 94 cases out of 100. \n",
      "However, it was able to capture only 33.0 % of the at-risk patients\n",
      "Why ? Let's print a confusion matrix!\n"
     ]
    }
   ],
   "source": [
    "print(f\"You should have noticed that the model was able to predict correctly in {int(round(correct_pred_ratio,2)*100)} cases out of 100. \")\n",
    "\n",
    "print(f\"However, it was able to capture only {round(flag_ratio,2)*100} % of the at-risk patients\")\n",
    "\n",
    "print(\"Why ? Let's print a confusion matrix!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ Using `plot_confusion_matrix` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)),  visualize the predictions breakdown of the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hints</summary>\n",
    "\n",
    "    \n",
    "1. [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)    \n",
    "2. ['ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)\n",
    "    \n",
    "- Don't forget to to go back to the **Holdout method** to [`train-test-split`]((https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) your dataset and look at the confusion matrix on the test set.  \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17910,   207],\n",
       "       [  939,   509]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y\n",
    "y_pred = model.fit(X, y).predict(X)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f95c9f385e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dip = ConfusionMatrixDisplay(cm)\n",
    "dip.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bitazaratustra/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f95e28bdd90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â„¹ï¸ The confusion matrix should show that the model is influenced by the class imbalance: it predicts the heartbeats to be healthy most of the time. Due to this behaviour, the model is often correct and has a **high accuracy**. However, this also causes it to miss out on many at-risk heartbeats: it has **bad recall**...\n",
    "\n",
    "ðŸ‘‰ This model is therefore poor at the task of **flagging at-risk observations**.\n",
    "\n",
    "â—ï¸ Don't be fooled by the accuracy and look at the metric that corresponds to your task! â—ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Classification Model Selection)** â“ \n",
    "\n",
    "Would a default KNN classifier perform better at the task of flagging at-risk observations?\n",
    "\n",
    "Save the you answer under `best_model` as \"KNN\" or \"LogisticRegression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recall(log_reg) = 0.3300942608280635Recall(KNN) = 0.8680507662835248Precision(log_reg) = 0.6867386740061804Precision(KNN) = 0.9438330326255999Accuracy(log_reg) = 0.9391771019677997Accuracy2(KNN) = 0.9864040573767493f1(log_reg) = 0.44482198050033483f12(KNN) = 0.9041585493379827\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "cv_results2 = cross_validate(model2, X, y, cv=10, scoring=['accuracy', 'precision', 'f1','recall'])\n",
    "Accuracy2 = cv_results2['test_accuracy'].mean()\n",
    "Recall2 = cv_results2['test_recall'].mean()\n",
    "Precision2 = cv_results2['test_precision'].mean()\n",
    "F12 = cv_results2['test_f1'].mean()\n",
    "\n",
    "y_true = y\n",
    "y_pred2 = model2.fit(X, y).predict(X)\n",
    "cm = confusion_matrix(y_true, y_pred2)\n",
    "cm\n",
    "\n",
    "print(f' Recall(log_reg) = {Recall}'\n",
    "         f'Recall(KNN) = {Recall2}'\n",
    "         f'Precision(log_reg) = {Precision}'\n",
    "         f'Precision(KNN) = {Precision2}'\n",
    "         f'Accuracy(log_reg) = {Accuracy}'\n",
    "         f'Accuracy2(KNN) = {Accuracy2}'\n",
    "         f'f1(log_reg) = {F1}'\n",
    "         f'f12(KNN) = {F12}')\n",
    "\n",
    "best_model = 'KNN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’ª For this ECG dataset, the KNN Classifier should have a much higher recall than the LogisticRegression and therefore is better suited for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/bitazaratustra/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/bitazaratustra/code/bitazaratustra/data-challenges/05-ML/03-Performance-metrics/02-Electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_best_model.py::TestBest_model::test_best_model \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/best_model.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed best_model step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('best_model',\n",
    "                         model = best_model)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected the KNN model thanks to its higherbest recall, let's have a look at the other classification performance metrics>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Classification Report)** â“\n",
    "\n",
    "Print out a [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) of the KNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> ðŸ’¡ <i>Hint</i>  </summary>\n",
    "    \n",
    "* You will need to pass the predictions of the model to a `classification_report`.\n",
    "    \n",
    "* SkLearn's [`cross_val_predict`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) might help ðŸ˜‰\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     18117\n",
      "           1       0.94      0.87      0.90      1448\n",
      "\n",
      "    accuracy                           0.99     19565\n",
      "   macro avg       0.97      0.93      0.95     19565\n",
      "weighted avg       0.99      0.99      0.99     19565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_final = cross_val_predict(model2, X, y, cv=10)\n",
    "report = classification_report(y, y_pred_final)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Reading the report)** â“\n",
    "\n",
    "\n",
    "Among the heartbeats predicted at-risk, what is the ratio of correct predictions ? \n",
    "\n",
    "In mathematical terms, can you read the ratio $ \\frac{TP}{TP + FP} $ in the report? What is the name of this classification metrics ? \n",
    "\n",
    "Save your answer as a float under `correct_at_risk_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9306"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.99*0.94/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "correct_atrisk_predictions = 0.99*0.94/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/bitazaratustra/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/bitazaratustra/code/bitazaratustra/data-challenges/05-ML/03-Performance-metrics/02-Electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_precision.py::TestPrecision::test_precision \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/precision.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed precision step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('precision',\n",
    "                         precision = correct_at_risk_predictions)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Predicting)** â“\n",
    "\n",
    "A patient comes to you for a second opinion because  he was told that based on his heartbeats, this patient may be at-risk.  \n",
    "\n",
    "According to your optimal model, is he at-risk or not?  \n",
    "\n",
    "Save the prediction of your model under variable name `prediction` as \"at risk\" or \"healthy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>x_11</th>\n",
       "      <th>x_12</th>\n",
       "      <th>x_13</th>\n",
       "      <th>x_14</th>\n",
       "      <th>x_15</th>\n",
       "      <th>x_16</th>\n",
       "      <th>x_17</th>\n",
       "      <th>x_18</th>\n",
       "      <th>x_19</th>\n",
       "      <th>x_20</th>\n",
       "      <th>x_21</th>\n",
       "      <th>x_22</th>\n",
       "      <th>x_23</th>\n",
       "      <th>x_24</th>\n",
       "      <th>x_25</th>\n",
       "      <th>x_26</th>\n",
       "      <th>x_27</th>\n",
       "      <th>x_28</th>\n",
       "      <th>x_29</th>\n",
       "      <th>x_30</th>\n",
       "      <th>x_31</th>\n",
       "      <th>x_32</th>\n",
       "      <th>x_33</th>\n",
       "      <th>x_34</th>\n",
       "      <th>x_35</th>\n",
       "      <th>x_36</th>\n",
       "      <th>x_37</th>\n",
       "      <th>x_38</th>\n",
       "      <th>x_39</th>\n",
       "      <th>x_40</th>\n",
       "      <th>x_41</th>\n",
       "      <th>x_42</th>\n",
       "      <th>x_43</th>\n",
       "      <th>x_44</th>\n",
       "      <th>x_45</th>\n",
       "      <th>x_46</th>\n",
       "      <th>x_47</th>\n",
       "      <th>x_48</th>\n",
       "      <th>x_49</th>\n",
       "      <th>x_50</th>\n",
       "      <th>x_51</th>\n",
       "      <th>x_52</th>\n",
       "      <th>x_53</th>\n",
       "      <th>x_54</th>\n",
       "      <th>x_55</th>\n",
       "      <th>x_56</th>\n",
       "      <th>x_57</th>\n",
       "      <th>x_58</th>\n",
       "      <th>x_59</th>\n",
       "      <th>x_60</th>\n",
       "      <th>x_61</th>\n",
       "      <th>x_62</th>\n",
       "      <th>x_63</th>\n",
       "      <th>x_64</th>\n",
       "      <th>x_65</th>\n",
       "      <th>x_66</th>\n",
       "      <th>x_67</th>\n",
       "      <th>x_68</th>\n",
       "      <th>x_69</th>\n",
       "      <th>x_70</th>\n",
       "      <th>x_71</th>\n",
       "      <th>x_72</th>\n",
       "      <th>x_73</th>\n",
       "      <th>x_74</th>\n",
       "      <th>x_75</th>\n",
       "      <th>x_76</th>\n",
       "      <th>x_77</th>\n",
       "      <th>x_78</th>\n",
       "      <th>x_79</th>\n",
       "      <th>x_80</th>\n",
       "      <th>x_81</th>\n",
       "      <th>x_82</th>\n",
       "      <th>x_83</th>\n",
       "      <th>x_84</th>\n",
       "      <th>x_85</th>\n",
       "      <th>x_86</th>\n",
       "      <th>x_87</th>\n",
       "      <th>x_88</th>\n",
       "      <th>x_89</th>\n",
       "      <th>x_90</th>\n",
       "      <th>x_91</th>\n",
       "      <th>x_92</th>\n",
       "      <th>x_93</th>\n",
       "      <th>x_94</th>\n",
       "      <th>x_95</th>\n",
       "      <th>x_96</th>\n",
       "      <th>x_97</th>\n",
       "      <th>x_98</th>\n",
       "      <th>x_99</th>\n",
       "      <th>x_100</th>\n",
       "      <th>x_101</th>\n",
       "      <th>x_102</th>\n",
       "      <th>x_103</th>\n",
       "      <th>x_104</th>\n",
       "      <th>x_105</th>\n",
       "      <th>x_106</th>\n",
       "      <th>x_107</th>\n",
       "      <th>x_108</th>\n",
       "      <th>x_109</th>\n",
       "      <th>x_110</th>\n",
       "      <th>x_111</th>\n",
       "      <th>x_112</th>\n",
       "      <th>x_113</th>\n",
       "      <th>x_114</th>\n",
       "      <th>x_115</th>\n",
       "      <th>x_116</th>\n",
       "      <th>x_117</th>\n",
       "      <th>x_118</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "      <th>x_128</th>\n",
       "      <th>x_129</th>\n",
       "      <th>x_130</th>\n",
       "      <th>x_131</th>\n",
       "      <th>x_132</th>\n",
       "      <th>x_133</th>\n",
       "      <th>x_134</th>\n",
       "      <th>x_135</th>\n",
       "      <th>x_136</th>\n",
       "      <th>x_137</th>\n",
       "      <th>x_138</th>\n",
       "      <th>x_139</th>\n",
       "      <th>x_140</th>\n",
       "      <th>x_141</th>\n",
       "      <th>x_142</th>\n",
       "      <th>x_143</th>\n",
       "      <th>x_144</th>\n",
       "      <th>x_145</th>\n",
       "      <th>x_146</th>\n",
       "      <th>x_147</th>\n",
       "      <th>x_148</th>\n",
       "      <th>x_149</th>\n",
       "      <th>x_150</th>\n",
       "      <th>x_151</th>\n",
       "      <th>x_152</th>\n",
       "      <th>x_153</th>\n",
       "      <th>x_154</th>\n",
       "      <th>x_155</th>\n",
       "      <th>x_156</th>\n",
       "      <th>x_157</th>\n",
       "      <th>x_158</th>\n",
       "      <th>x_159</th>\n",
       "      <th>x_160</th>\n",
       "      <th>x_161</th>\n",
       "      <th>x_162</th>\n",
       "      <th>x_163</th>\n",
       "      <th>x_164</th>\n",
       "      <th>x_165</th>\n",
       "      <th>x_166</th>\n",
       "      <th>x_167</th>\n",
       "      <th>x_168</th>\n",
       "      <th>x_169</th>\n",
       "      <th>x_170</th>\n",
       "      <th>x_171</th>\n",
       "      <th>x_172</th>\n",
       "      <th>x_173</th>\n",
       "      <th>x_174</th>\n",
       "      <th>x_175</th>\n",
       "      <th>x_176</th>\n",
       "      <th>x_177</th>\n",
       "      <th>x_178</th>\n",
       "      <th>x_179</th>\n",
       "      <th>x_180</th>\n",
       "      <th>x_181</th>\n",
       "      <th>x_182</th>\n",
       "      <th>x_183</th>\n",
       "      <th>x_184</th>\n",
       "      <th>x_185</th>\n",
       "      <th>x_186</th>\n",
       "      <th>x_187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.993197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956916</td>\n",
       "      <td>0.902494</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.709751</td>\n",
       "      <td>0.557823</td>\n",
       "      <td>0.321995</td>\n",
       "      <td>0.192744</td>\n",
       "      <td>0.147392</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.00907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011338</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.054422</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.113379</td>\n",
       "      <td>0.160998</td>\n",
       "      <td>0.185941</td>\n",
       "      <td>0.208617</td>\n",
       "      <td>0.219955</td>\n",
       "      <td>0.240363</td>\n",
       "      <td>0.231293</td>\n",
       "      <td>0.226757</td>\n",
       "      <td>0.231293</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.235828</td>\n",
       "      <td>0.235828</td>\n",
       "      <td>0.24263</td>\n",
       "      <td>0.249433</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.256236</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.276644</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.292517</td>\n",
       "      <td>0.303855</td>\n",
       "      <td>0.321995</td>\n",
       "      <td>0.337868</td>\n",
       "      <td>0.337868</td>\n",
       "      <td>0.340136</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.297052</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.371882</td>\n",
       "      <td>0.639456</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.807256</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.29932</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.278912</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.251701</td>\n",
       "      <td>0.256236</td>\n",
       "      <td>0.247166</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2  x_3       x_4       x_5       x_6       x_7       x_8  \\\n",
       "0  0.904762  0.993197  1.0  0.956916  0.902494  0.857143  0.802721  0.777778   \n",
       "\n",
       "        x_9      x_10      x_11      x_12      x_13      x_14      x_15  \\\n",
       "0  0.709751  0.557823  0.321995  0.192744  0.147392  0.129252  0.099773   \n",
       "\n",
       "       x_16      x_17      x_18      x_19      x_20      x_21      x_22  \\\n",
       "0  0.092971  0.068027  0.068027  0.061224  0.040816  0.034014  0.027211   \n",
       "\n",
       "       x_23     x_24  x_25      x_26  x_27      x_28      x_29      x_30  \\\n",
       "0  0.013605  0.00907   0.0  0.006803   0.0  0.011338  0.015873  0.031746   \n",
       "\n",
       "       x_31      x_32      x_33      x_34      x_35      x_36      x_37  \\\n",
       "0  0.054422  0.092971  0.113379  0.160998  0.185941  0.208617  0.219955   \n",
       "\n",
       "       x_38      x_39      x_40      x_41      x_42      x_43      x_44  \\\n",
       "0  0.240363  0.231293  0.226757  0.231293  0.238095  0.235828  0.235828   \n",
       "\n",
       "      x_45      x_46      x_47      x_48      x_49      x_50      x_51  \\\n",
       "0  0.24263  0.249433  0.253968  0.258503  0.258503  0.256236  0.253968   \n",
       "\n",
       "       x_52      x_53      x_54      x_55      x_56      x_57      x_58  \\\n",
       "0  0.265306  0.263039  0.272109  0.265306  0.260771  0.263039  0.267574   \n",
       "\n",
       "       x_59      x_60      x_61      x_62      x_63      x_64      x_65  \\\n",
       "0  0.267574  0.274376  0.258503  0.265306  0.263039  0.267574  0.272109   \n",
       "\n",
       "       x_66      x_67      x_68      x_69      x_70      x_71      x_72  \\\n",
       "0  0.263039  0.260771  0.274376  0.269841  0.274376  0.276644  0.269841   \n",
       "\n",
       "       x_73      x_74      x_75      x_76      x_77      x_78      x_79  \\\n",
       "0  0.267574  0.274376  0.292517  0.303855  0.321995  0.337868  0.337868   \n",
       "\n",
       "       x_80      x_81      x_82      x_83      x_84      x_85      x_86  \\\n",
       "0  0.340136  0.319728  0.297052  0.285714  0.269841  0.269841  0.274376   \n",
       "\n",
       "       x_87      x_88      x_89      x_90      x_91      x_92      x_93  \\\n",
       "0  0.269841  0.274376  0.267574  0.260771  0.371882  0.639456  0.959184   \n",
       "\n",
       "       x_94      x_95     x_96      x_97      x_98      x_99     x_100  \\\n",
       "0  0.807256  0.444444  0.29932  0.272109  0.278912  0.253968  0.258503   \n",
       "\n",
       "      x_101     x_102     x_103     x_104     x_105     x_106  x_107  x_108  \\\n",
       "0  0.251701  0.256236  0.247166  0.265306  0.265306  0.267574    0.0    0.0   \n",
       "\n",
       "   x_109  x_110  x_111  x_112  x_113  x_114  x_115  x_116  x_117  x_118  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_119  x_120  x_121  x_122  x_123  x_124  x_125  x_126  x_127  x_128  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_129  x_130  x_131  x_132  x_133  x_134  x_135  x_136  x_137  x_138  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_139  x_140  x_141  x_142  x_143  x_144  x_145  x_146  x_147  x_148  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_149  x_150  x_151  x_152  x_153  x_154  x_155  x_156  x_157  x_158  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_159  x_160  x_161  x_162  x_163  x_164  x_165  x_166  x_167  x_168  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_169  x_170  x_171  x_172  x_173  x_174  x_175  x_176  x_177  x_178  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_179  x_180  x_181  x_182  x_183  x_184  x_185  x_186  x_187  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_patient = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Electrocardiograms_new_patient.csv')\n",
    "new_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at risk'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "pred = model2.predict(new_patient)\n",
    "if pred[0] == 1:\n",
    "    prediction = 'at risk'\n",
    "else :\n",
    "    prediction = 'healthy'\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/bitazaratustra/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/bitazaratustra/code/bitazaratustra/data-challenges/05-ML/03-Performance-metrics/02-Electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_prediction.py::TestPrediction::test_prediction_at_risk \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/prediction.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed prediction step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('prediction',\n",
    "                         prediction = prediction)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ Congratulations!\n",
    "\n",
    "ðŸ’¾ Don't forget to git add/commit/push your notebook...\n",
    "\n",
    "ðŸš€ ... and move on to the next challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
