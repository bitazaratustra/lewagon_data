{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(X_train.shape)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X_train[0], cmap=\"gray\");\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X_train[1], cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "normalized_X_train = np.array([n/255 for n in X_train])\n",
    "normalized_X_test = np.array([n/255 for n in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "normalized_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 13:41:02.924025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-26 13:41:02.960508: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/bitazaratustra/miniconda3/lib/\n",
      "2022-09-26 13:41:02.960549: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-09-26 13:41:02.962878: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-26 13:41:02.964404: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 62720000 exceeds 10% of free system memory.\n",
      "2022-09-26 13:41:03.003015: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 376320000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "normalized_X_test = expand_dims(normalized_X_test, axis=-1, )\n",
    "normalized_X_train = expand_dims(normalized_X_train, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_cat_train = to_categorical(y_train, num_classes=10)\n",
    "y_cat_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(28, 28, 1), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3,3), input_shape=(28, 28, 1), activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
    "    \n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    \n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    ### Model compilation\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 12, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5770      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f30046f3df0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "model.fit(normalized_X_train, y_cat_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,  \n",
    "          validation_split=0.3,\n",
    "          callbacks=[es],\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0551 - accuracy: 0.9819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.055086199194192886, 0.9818999767303467]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "model.evaluate(normalized_X_test, y_cat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
